{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Two Tower Model\n",
    "\n",
    "This is the idea I suggested in the meeting. In it you have two embeddings: one for the users and one for the nudge. Their models can be changed and adapted over time, as long as the embedding stays the same. The matches between users and nudges can then be ranked by distance, which can be biased to suit our needs. The best approach for learning is to do it in daily cycles, the content still changes throughout the day as user input changes and geolocation changes but the model itself does not.\n",
    "\n",
    "## Towers\n",
    "\n",
    "These towers can be any trainable model as shown in an example below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class TowerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, **config):\n",
    "        super(TowerModel, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.model1 = nn.Sequential(nn.Linear(config['input_dim_1'], config['hidden_dim_1']),\n",
    "                                    *[nn.Softmax(dim=-1), nn.Linear(config['hidden_dim_1'], config['hidden_dim_1'])]*config['hidden_layers_1'],\n",
    "                                    nn.ReLU())\n",
    "        self.model2 = nn.Sequential(nn.Linear(config['input_dim_2'], config['hidden_dim_2']),\n",
    "                                    *[nn.Softmax(dim=-1), nn.Linear(config['hidden_dim_2'], config['hidden_dim_2'])]*config['hidden_layers_2'],\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.readout = nn.Linear(config['hidden_dim_1']+config['hidden_dim_2'], config['output_dim'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        look at different user data using different models and then produce an embeddign with the combined inputs\n",
    "        :param input:\n",
    "        :return: embedding\n",
    "        \"\"\"\n",
    "        out1 = self.model1(input[..., :self.config['input_dim_1']])\n",
    "        out2 = self.model2(input[..., self.config['input_dim_1']:])\n",
    "\n",
    "        return self.readout(torch.concat((out1, out2), dim=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see later that the models chosen are arbitrary, provided `config['output_dim']` remains unchanged. Most typically one would use different DNN networks to embed different data pieces and then combine their outputs. The best example of this may be to embed metadata of the nudges and separately embed the text.\n",
    "\n",
    "## Combined\n",
    "\n",
    "Below is how the model would look when set-up in the Two-Tower model. As we can see, the model takes in batches of users and nudges and rates them against each other. These ratings can then be used to create a ranking of nudges for each user. (I strongly recommend that this ranking is injected with some less ``good'' nudges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Two tower model for offline ranking of nudges against users (both are reduced in complexity through embedding).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **config):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "\n",
    "        self.user_tower = TowerModel(**config)\n",
    "        self.nudge_tower = TowerModel(**config)\n",
    "\n",
    "    def forward(self, **inputs) -> torch.tensor:\n",
    "        user_embedding = self.user_tower(inputs['user_input'])\n",
    "        nudge_embedding = self.nudge_tower(inputs['nudge_input'])\n",
    "        return torch.matmul(user_embedding, nudge_embedding.T) # compute the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Gate Mixture of Experts\n",
    "\n",
    "For many tasks we may have different objectives we would seek to optimise. For example, we may be interested in maximising user interaction number and duration. These are different tasks, which may have different optimal solutions.\n",
    "\n",
    "Heuristically, to target this, we may choose to pass the choices and information to different networks targeting these different objectives. These networks are known as experts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def Expert(**config) -> nn.Module:\n",
    "    embedding_layer = nn.Linear(config['input_dim'], config['exp_hidden_dim'])\n",
    "    hidden_layer = nn.Linear(config['exp_hidden_dim'], config['exp_hidden_dim'])\n",
    "    output_layer = nn.Linear(config['exp_hidden_dim'], config['output_dim'])\n",
    "    return nn.Sequential(*[embedding_layer, nn.Sigmoid(),\n",
    "                           *[hidden_layer, nn.ReLU()] * config['exp_layers'],\n",
    "                           output_layer])\n",
    "\n",
    "\n",
    "def Gate(**config) -> nn.Module:\n",
    "    embedding_layer = nn.Linear(config['input_dim'], config['gate_hidden_dim'])\n",
    "    return nn.Sequential(embedding_layer, nn.Sigmoid(),\n",
    "                         nn.Linear(config['gate_hidden_dim'], config['exp_num']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class MultiGateMixExperts(nn.Module):\n",
    "\n",
    "    def __init__(self, **config):\n",
    "        super(MultiGateMixExperts, self).__init__()\n",
    "\n",
    "        self.input_dim = config['input_dim']\n",
    "\n",
    "        self.gate1 = Gate(**config)\n",
    "        self.gate2 = Gate(**config)\n",
    "\n",
    "        self.experts = [Expert(**config) for _ in config['exp_num']]\n",
    "\n",
    "        self.utility1 = TowerModel(**config)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # input should be concatenation of:\n",
    "        # user id, distance vector in embedding, user and nudge data and geodata\n",
    "        f = torch.tensor([expert(inp) for expert in self.experts])\n",
    "        g1, g2 = self.gate1(inp), self.gate2(inp)\n",
    "        return torch.matmul(g1, f), torch.matmul(g2, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![An overview of the model](https://miro.medium.com/v2/resize:fit:914/format:webp/1*w0MonzJA7LMGUO2_Hcsd9w.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss\n",
    "\n",
    "The most common form of loss for nudge recommendation models is Cross Entropy Loss. In this we penalise unchosen options to take into account that a full evaluation of the preferences is not possible. This problem is addressed in a different way by the next method.\n",
    "\n",
    "Cross entropy loss takes into account the relative abundance of labels and then uses the options presented to the user to estimate the loss. Consider presenting 5 different nudges to a user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class CustomCrossEntropyLoss:\n",
    "\n",
    "    \"\"\"\n",
    "    The most commonly found strategy is called in-batch negative sampling: for a specific observation\n",
    "    in a batch we consider every other observation in this batch as negative. This is because a full\n",
    "    evaluation will not be possible.\n",
    "\n",
    "    :param label_probs: precompute the relative abundance of each label for the weighting\n",
    "    :return loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label_probs: torch.tensor):\n",
    "        self.label_probs = label_probs\n",
    "\n",
    "    def __call__(self, true_labels: torch.tensor, logits: torch.tensor, training: bool = False) -> torch.tensor:\n",
    "        batch_size, nb_candidates = logits.shape\n",
    "\n",
    "        if training:\n",
    "            label_probs = torch.zeros(true_labels.shape)\n",
    "            for label in true_labels:\n",
    "                label_probs[true_labels == label] = self.label_probs[label] * torch.ones(true_labels.shape)\n",
    "            logits -= torch.log(label_probs)\n",
    "\n",
    "            true_labels = torch.range(0, batch_size)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(logits, true_labels)\n",
    "        return torch.sum(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- https://dl.acm.org/doi/pdf/10.1145/3219819.3220007\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Swing Algorithm\n",
    "\n",
    "This algorithm is designed for online running and on the Alibaba website 1688 it has had a 72% click-through rate.\n",
    "\n",
    "The following scoring system is used for candidate generation. We use the score function below to rate user-nudge combinations.\n",
    "\n",
    "$w_u = \\frac{1}{(cnt+5)^{0.35}}$\n",
    "\n",
    "This is the user weighting, where $c, n$ and $t$ are user-interaction counts.\n",
    "\n",
    "$w_\\text{pair} = w_u * w_\\tilde{u}$\n",
    "\n",
    "These are the pair weightings.\n",
    "\n",
    "score$(i,j)= \\sum_\\text{pairs} \\frac{w_\\text{pair}}{1+\\text{intersection}}$\n",
    "\n",
    "This score function considers the intersection of both user interactions. It can easily be implemented as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "u2items = [[0, 17, 3, 24, 135], [0, 3, 53, 392, 24, 2]]         # u2items = array of users and their items\n",
    "# u2items[i] = items user i clicked on\n",
    "# u2items[j] = items user j clicked on\n",
    "\n",
    "i2i = {}\n",
    "\n",
    "for i in range(0, len(u2items)):\n",
    "    wi = math.pow(len(u2items[i]) + 5, -0.35)\n",
    "    for j in range(i + 1, len(u2items)):\n",
    "        intersection = u2items[i] and u2items[j]   # intersection = items both user i and user j clicked on\n",
    "        wj = wi * math.pow(len(u2items[j]) + 5, -0.35)  # wj = product-pair score\n",
    "        for product_id in intersection:\n",
    "            i2i[product_id] = i2i.get(product_id, 0.0) + wj / (1 + len(intersection))   # i2i is incrementally updated as we loop through users (we won't use a loop in production)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ranking done afterwards is a much more sensitive task. We have traded off accuracy for speed in candidate generation. This final step is the most model dependent and we need to ensure fast computation. Common examples used, include gradient boosted trees and deep learning models such as Multi gate mixture of exports. Ranking can be framed as either a classification or learning to rank problem. As a classification problem, we can score candidates based on probability of click or purchase. Logistic regression with crossed features is simple to implement and a difficult baseline to beat. Decision trees are also commonly used. As a learning to rank problem, commonly used algorithms include LambdaMart, XGBoost, and LightGBM.\n",
    "\n",
    "Online candidate generation is very intensive and often can be circumnavigated. The advantages of this approach test on its adaptability to user flows, but this can be accounted for with adaptive recommendation. In our case, with few nudges, we donâ€™t need to be able to generate large collections of candidates from even larger samples. We can rely on generating a couple dozen candidates per user per day."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "- https://eugeneyan.com/writing/real-time-recommendations/"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
